{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "toc_visible": true,
      "collapsed_sections": [
        "xO_ZeF8rWrop"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Git Clone for Dataset & Pickel Files"
      ],
      "metadata": {
        "id": "xO_ZeF8rWrop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We cloned the GitHub repository and moved to the correct directory\n",
        "!git clone https://github.com/Yogesh0903/Phishing_URL.git\n",
        "!ls Phishing_URL/Model_7  # We checked if models exist\n"
      ],
      "metadata": {
        "id": "AXAVNvLFV_1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89be9e67-28da-425a-abb6-8023c0337613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Phishing_URL'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 38 (delta 8), reused 3 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (38/38), 31.44 MiB | 9.85 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n",
            "decision_tree.pkl\t logistic_regression.pkl  neural_network.pkl  svm.pkl\n",
            "k-nearest_neighbors.pkl  naive_bayes.pkl\t  random_forest.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "8lTWOEWKWtrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.somaiya.edu/en/\n"
      ],
      "metadata": {
        "id": "dpgyvAhsZZXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.meesho.com/"
      ],
      "metadata": {
        "id": "My3qcZnOaSZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/kamranahmedse"
      ],
      "metadata": {
        "id": "o7ewrhTka-om"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.jnj.com/"
      ],
      "metadata": {
        "id": "Flz6Xfc-bMM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import pickle\n",
        "import numpy as np\n",
        "import statistics\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Function to extract features from a URL\n",
        "def extract_features(url):\n",
        "    features = {\n",
        "        'LineOfCode': 0,\n",
        "        'LargestLineLength': 0,\n",
        "        'NoOfSelfRef': 0,\n",
        "        'IsHTTPS': 1 if url.startswith(\"https\") else 0,\n",
        "        'NoOfExternalRef': 0,\n",
        "        'NoOfImage': 0,\n",
        "        'NoOfJS': 0\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        for _ in range(3):  # We retried up to 3 times\n",
        "            response = requests.get(url, timeout=5)\n",
        "            if response.status_code == 200:\n",
        "                break\n",
        "            time.sleep(2)\n",
        "        else:\n",
        "            print(f\"Error fetching the webpage: {response.status_code} {response.reason}\")\n",
        "            return features\n",
        "\n",
        "        response.raise_for_status()\n",
        "\n",
        "        lines = response.text.split('\\n')\n",
        "        features['LineOfCode'] = len(lines)\n",
        "        features['LargestLineLength'] = max(len(line) for line in lines)\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        domain = urlparse(url).netloc\n",
        "\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link['href']\n",
        "            full_url = urljoin(url, href)\n",
        "            if urlparse(full_url).netloc == domain:\n",
        "                features['NoOfSelfRef'] += 1\n",
        "            else:\n",
        "                features['NoOfExternalRef'] += 1\n",
        "\n",
        "        features['NoOfImage'] = len(soup.find_all('img'))\n",
        "        features['NoOfJS'] = len(soup.find_all('script'))\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching the webpage: {e}\")\n",
        "\n",
        "    return features\n",
        "\n",
        "# Function to load models from 'Model_7' folder\n",
        "def load_models():\n",
        "    models = {}\n",
        "    model_names = [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\",\n",
        "                   \"K-Nearest Neighbors\", \"Naive Bayes\", \"Neural Network\", \"SVM\"]\n",
        "\n",
        "    model_folder = \"Phishing_URL/Model_7/\"  # We set the path to model files\n",
        "\n",
        "    for name in model_names:\n",
        "        filename = f\"{model_folder}{name.replace(' ', '_').lower()}.pkl\"\n",
        "        if os.path.exists(filename):  # We checked if the file exists\n",
        "            try:\n",
        "                with open(filename, 'rb') as f:\n",
        "                    models[name] = pickle.load(f)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: {filename} not found. We skipped this model.\")\n",
        "\n",
        "    return models\n",
        "\n",
        "# Function to predict whether a URL is phishing or legitimate\n",
        "def predict_url(url, models):\n",
        "    features = extract_features(url)\n",
        "    feature_values = np.array(list(features.values())).reshape(1, -1)\n",
        "\n",
        "    predictions = []\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            pred = model.predict(feature_values)[0]\n",
        "            predictions.append(pred)\n",
        "            print(f\"{name} Prediction: {'Legitimate' if pred else 'Phishing'}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error predicting with {name}: {e}\")\n",
        "\n",
        "    if predictions:\n",
        "        avg_prediction = np.mean(predictions)\n",
        "        mode_prediction = statistics.mode(predictions)\n",
        "\n",
        "        final_prediction = 'Legitimate' if mode_prediction == 1 else 'Phishing'\n",
        "        print(f\"\\nAverage Prediction: {avg_prediction:.4f}\")\n",
        "        print(f\"Mode Prediction: {mode_prediction}\")\n",
        "        print(f\"Final Prediction (based on mode): {final_prediction}\")\n",
        "    else:\n",
        "        print(\"No models available for prediction.\")\n",
        "\n",
        "# We loaded models\n",
        "models = load_models()\n",
        "\n",
        "# We got URL input from the user\n",
        "url = input(\"Enter URL to classify: \")\n",
        "predict_url(url, models)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5-7H1-oVJFr",
        "outputId": "870e852a-5320-4f18-ce18-aeecb2b7cfa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter URL to classify: https://www.somaiya.edu/en/\n",
            "Logistic Regression Prediction: Legitimate\n",
            "Decision Tree Prediction: Legitimate\n",
            "Random Forest Prediction: Legitimate\n",
            "K-Nearest Neighbors Prediction: Legitimate\n",
            "Naive Bayes Prediction: Legitimate\n",
            "Neural Network Prediction: Legitimate\n",
            "SVM Prediction: Legitimate\n",
            "\n",
            "Average Prediction: 1.0000\n",
            "Mode Prediction: 1\n",
            "Final Prediction (based on mode): Legitimate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rough Code"
      ],
      "metadata": {
        "id": "ETmu5EyrVODR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working Rough"
      ],
      "metadata": {
        "id": "5avhUkDbVQlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 12 Features"
      ],
      "metadata": {
        "id": "U60WK2AuHbRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import pickle\n",
        "import numpy as np\n",
        "import statistics\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Function to extract features from a URL\n",
        "def extract_features(url):\n",
        "    features = {\n",
        "        'LineOfCode': 0,\n",
        "        'LargestLineLength': 0,\n",
        "        'NoOfSelfRef': 0,\n",
        "        'IsHTTPS': 1 if url.startswith(\"https\") else 0,\n",
        "        'NoOfExternalRef': 0,\n",
        "        'NoOfImage': 0,\n",
        "        'NoOfJS': 0,\n",
        "        'NoOfCSS': 0,\n",
        "        'HasSocialNet': 0,\n",
        "        'DigitRatioInURL': sum(c.isdigit() for c in url) / len(url),\n",
        "        'NoOfSubDomain': urlparse(url).netloc.count('.'),\n",
        "        'HasCopyrightInfo': 0\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        for _ in range(3):  # Retry up to 3 times\n",
        "            response = requests.get(url, timeout=5)\n",
        "            if response.status_code == 200:\n",
        "                break\n",
        "            time.sleep(2)\n",
        "        else:\n",
        "            print(f\"Error fetching the webpage: {response.status_code} {response.reason}\")\n",
        "            return features\n",
        "\n",
        "        response.raise_for_status()\n",
        "\n",
        "        lines = response.text.split('\\n')\n",
        "        features['LineOfCode'] = len(lines)\n",
        "        features['LargestLineLength'] = max(len(line) for line in lines)\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        domain = urlparse(url).netloc\n",
        "\n",
        "        social_sites = ['facebook.com', 'twitter.com', 'instagram.com', 'linkedin.com', 'youtube.com']\n",
        "        copyright_keywords = ['copyright', 'Â©', 'all rights reserved']\n",
        "\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link['href']\n",
        "            full_url = urljoin(url, href)\n",
        "            if urlparse(full_url).netloc == domain:\n",
        "                features['NoOfSelfRef'] += 1\n",
        "            else:\n",
        "                features['NoOfExternalRef'] += 1\n",
        "                if any(site in full_url for site in social_sites):\n",
        "                    features['HasSocialNet'] = 1\n",
        "\n",
        "        features['NoOfImage'] = len(soup.find_all('img'))\n",
        "        features['NoOfJS'] = len(soup.find_all('script'))\n",
        "        features['NoOfCSS'] = len(soup.find_all('link', {'rel': 'stylesheet'}))\n",
        "\n",
        "        page_text = soup.get_text().lower()\n",
        "        features['HasCopyrightInfo'] = 1 if any(word in page_text for word in copyright_keywords) else 0\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching the webpage: {e}\")\n",
        "\n",
        "    return features\n",
        "\n",
        "# Function to load models from 'Model_12' folder\n",
        "def load_models():\n",
        "    models = {}\n",
        "    model_names = [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\",\n",
        "                   \"K-Nearest Neighbors\", \"Naive Bayes\", \"Neural Network\", \"SVM\"]\n",
        "\n",
        "    model_folder = \"Phishing_URL/Model_12/\"  # Path to model files\n",
        "\n",
        "    for name in model_names:\n",
        "        filename = f\"{model_folder}{name.replace(' ', '_').lower()}.pkl\"\n",
        "        if os.path.exists(filename):  # Check if the file exists\n",
        "            try:\n",
        "                with open(filename, 'rb') as f:\n",
        "                    models[name] = pickle.load(f)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: {filename} not found. Skipping this model.\")\n",
        "\n",
        "    return models\n",
        "\n",
        "# Function to predict whether a URL is phishing or legitimate\n",
        "def predict_url(url, models):\n",
        "    features = extract_features(url)\n",
        "    feature_values = np.array(list(features.values())).reshape(1, -1)\n",
        "\n",
        "    predictions = []\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            pred = model.predict(feature_values)[0]\n",
        "            predictions.append(pred)\n",
        "            print(f\"{name} Prediction: {'Legitimate' if pred else 'Phishing'}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error predicting with {name}: {e}\")\n",
        "\n",
        "    if predictions:\n",
        "        avg_prediction = np.mean(predictions)\n",
        "        mode_prediction = statistics.mode(predictions)\n",
        "\n",
        "        final_prediction = 'Legitimate' if mode_prediction == 1 else 'Phishing'\n",
        "        print(f\"\\nAverage Prediction: {avg_prediction:.4f}\")\n",
        "        print(f\"Mode Prediction: {mode_prediction}\")\n",
        "        print(f\"Final Prediction (based on mode): {final_prediction}\")\n",
        "    else:\n",
        "        print(\"No models available for prediction.\")\n",
        "\n",
        "# Load models from Model_12\n",
        "models = load_models()\n",
        "\n",
        "# Get URL input from user\n",
        "url = input(\"Enter URL to classify: \")\n",
        "predict_url(url, models)\n"
      ],
      "metadata": {
        "id": "H4hPv9NSGlZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e15d20e-3b55-42a2-f392-4b19deb48d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter URL to classify: https://www.netflix.com/browse\n",
            "Logistic Regression Prediction: Phishing\n",
            "Decision Tree Prediction: Legitimate\n",
            "Random Forest Prediction: Phishing\n",
            "K-Nearest Neighbors Prediction: Legitimate\n",
            "Naive Bayes Prediction: Legitimate\n",
            "Neural Network Prediction: Phishing\n",
            "SVM Prediction: Phishing\n",
            "\n",
            "Average Prediction: 0.4286\n",
            "Mode Prediction: 0\n",
            "Final Prediction (based on mode): Phishing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Bayes Model"
      ],
      "metadata": {
        "id": "9XulDtUtcbE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import pickle\n",
        "import numpy as np\n",
        "import statistics\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Function to extract features from a URL\n",
        "def extract_features(url):\n",
        "    features = {\n",
        "        'LineOfCode': 0,\n",
        "        'LargestLineLength': 0,\n",
        "        'NoOfSelfRef': 0,\n",
        "        'IsHTTPS': 1 if url.startswith(\"https\") else 0,\n",
        "        'NoOfExternalRef': 0,\n",
        "        'NoOfImage': 0,\n",
        "        'NoOfJS': 0\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        for _ in range(3):  # We retried up to 3 times\n",
        "            response = requests.get(url, timeout=5)\n",
        "            if response.status_code == 200:\n",
        "                break\n",
        "            time.sleep(2)\n",
        "        else:\n",
        "            print(f\"Error fetching the webpage: {response.status_code} {response.reason}\")\n",
        "            return features\n",
        "\n",
        "        response.raise_for_status()\n",
        "\n",
        "        lines = response.text.split('\\n')\n",
        "        features['LineOfCode'] = len(lines)\n",
        "        features['LargestLineLength'] = max(len(line) for line in lines)\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        domain = urlparse(url).netloc\n",
        "\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link['href']\n",
        "            full_url = urljoin(url, href)\n",
        "            if urlparse(full_url).netloc == domain:\n",
        "                features['NoOfSelfRef'] += 1\n",
        "            else:\n",
        "                features['NoOfExternalRef'] += 1\n",
        "\n",
        "        features['NoOfImage'] = len(soup.find_all('img'))\n",
        "        features['NoOfJS'] = len(soup.find_all('script'))\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching the webpage: {e}\")\n",
        "\n",
        "    return features\n",
        "\n",
        "# Function to load models from 'Model_7' folder\n",
        "def load_models():\n",
        "    models = {}\n",
        "    model_names = [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\",\n",
        "                   \"K-Nearest Neighbors\", \"Naive Bayes\", \"Neural Network\", \"SVM\"]\n",
        "\n",
        "    model_folder = \"Phishing_URL/Model_7/\"  # We set the path to model files\n",
        "\n",
        "    for name in model_names:\n",
        "        filename = f\"{model_folder}{name.replace(' ', '_').lower()}.pkl\"\n",
        "        if os.path.exists(filename):  # We checked if the file exists\n",
        "            try:\n",
        "                with open(filename, 'rb') as f:\n",
        "                    models[name] = pickle.load(f)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: {filename} not found. We skipped this model.\")\n",
        "\n",
        "    return models\n",
        "\n",
        "# Function to predict whether a URL is phishing or legitimate\n",
        "def predict_url(url, models):\n",
        "    features = extract_features(url)\n",
        "    feature_values = np.array(list(features.values())).reshape(1, -1)\n",
        "\n",
        "    if \"Naive Bayes\" in models:\n",
        "        try:\n",
        "            pred = models[\"Naive Bayes\"].predict(feature_values)[0]\n",
        "            print(f\"Naive Bayes Prediction: {'Legitimate' if pred else 'Phishing'}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error predicting with Naive Bayes: {e}\")\n",
        "    else:\n",
        "        print(\"Naive Bayes model not found.\")\n",
        "\n",
        "\n",
        "# We loaded models\n",
        "models = load_models()\n",
        "\n",
        "# We got URL input from the user\n",
        "url = input(\"Enter URL to classify: \")\n",
        "predict_url(url, models)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTpvWJqCcdds",
        "outputId": "ac6f7a09-dc95-432b-e4ef-7219e5cc3068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter URL to classify: https://meet.google.com/btu-wpjo-jie\n",
            "Naive Bayes Prediction: Phishing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Others"
      ],
      "metadata": {
        "id": "VHH-r4XbVV_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import pickle\n",
        "import numpy as np\n",
        "import statistics\n",
        "import time\n",
        "\n",
        "def extract_features(url):\n",
        "    features = {\n",
        "        'LineOfCode': 0,\n",
        "        'LargestLineLength': 0,\n",
        "        'NoOfSelfRef': 0,\n",
        "        'IsHTTPS': 1 if url.startswith(\"https\") else 0,\n",
        "        'NoOfExternalRef': 0,\n",
        "        'NoOfImage': 0,\n",
        "        'NoOfJS': 0\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        for _ in range(3):  # Retry up to 3 times\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                break\n",
        "            time.sleep(2)\n",
        "        else:\n",
        "            print(f\"Error fetching the webpage: {response.status_code} {response.reason}\")\n",
        "            return features\n",
        "\n",
        "        response.raise_for_status()\n",
        "\n",
        "        lines = response.text.split('\\n')\n",
        "        features['LineOfCode'] = len(lines)\n",
        "        features['LargestLineLength'] = max(len(line) for line in lines)\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        domain = urlparse(url).netloc\n",
        "\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link['href']\n",
        "            full_url = urljoin(url, href)\n",
        "            if urlparse(full_url).netloc == domain:\n",
        "                features['NoOfSelfRef'] += 1\n",
        "            else:\n",
        "                features['NoOfExternalRef'] += 1\n",
        "\n",
        "        features['NoOfImage'] = len(soup.find_all('img'))\n",
        "        features['NoOfJS'] = len(soup.find_all('script'))\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching the webpage: {e}\")\n",
        "\n",
        "    return features\n",
        "\n",
        "def load_models():\n",
        "    models = {}\n",
        "    model_names = [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"K-Nearest Neighbors\", \"Naive Bayes\", \"Neural Network\", \"SVM\"]\n",
        "\n",
        "    for name in model_names:\n",
        "        filename = f\"{name.replace(' ', '_').lower()}.pkl\"\n",
        "        try:\n",
        "            with open(filename, 'rb') as f:\n",
        "                models[name] = pickle.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: {filename} not found. Skipping this model.\")\n",
        "\n",
        "    return models\n",
        "\n",
        "def predict_url(url, models):\n",
        "    features = extract_features(url)\n",
        "    feature_values = np.array(list(features.values())).reshape(1, -1)\n",
        "\n",
        "    predictions = []\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            pred = model.predict(feature_values)[0]\n",
        "            predictions.append(pred)\n",
        "            print(f\"{name} Prediction: {'Legitimate' if pred else 'Phishing'}\")  # Updated label mapping\n",
        "        except Exception as e:\n",
        "            print(f\"Error predicting with {name}: {e}\")\n",
        "\n",
        "    if predictions:\n",
        "        avg_prediction = np.mean(predictions)\n",
        "        mode_prediction = statistics.mode(predictions)\n",
        "\n",
        "        final_prediction = 'Legitimate' if mode_prediction == 1 else 'Phishing'  # Corrected final decision\n",
        "        print(f\"\\nAverage Prediction: {avg_prediction:.4f}\")\n",
        "        print(f\"Mode Prediction: {mode_prediction}\")\n",
        "        print(f\"Final Prediction (based on mode): {final_prediction}\")\n",
        "    else:\n",
        "        print(\"No models available for prediction.\")\n",
        "\n",
        "# Get URL from user and predict\n",
        "url = input(\"Enter URL to classify: \")\n",
        "models = load_models()\n",
        "predict_url(url, models)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHD-H78z6Tk4",
        "outputId": "5a3965eb-a25b-4fb0-edbf-0ced8f0bc736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter URL to classify: https://www.google.com/search?q=google&rlz=1C1RXQR_enIN979IN980&oq=google+&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIMCAEQIxgnGIAEGIoFMhIIAhAAGEMYgwEYsQMYgAQYigUyDQgDEAAYkQIYgAQYigUyEggEEAAYQxiDARixAxiABBiKBTITCAUQABiDARiRAhixAxiABBiKBTIPCAYQABhDGLEDGIAEGIoFMhIIBxAAGEMYgwEYsQMYgAQYigUyBwgIEAAYjwIyBwgJEAAYjwLSAQoxMDgxN2owajE1qAIJsAIB8QXe6bonfUVCnQ&sourceid=chrome&ie=UTF-8\n",
            "Logistic Regression Prediction: Phishing\n",
            "Decision Tree Prediction: Phishing\n",
            "Random Forest Prediction: Phishing\n",
            "K-Nearest Neighbors Prediction: Phishing\n",
            "Naive Bayes Prediction: Phishing\n",
            "Neural Network Prediction: Phishing\n",
            "SVM Prediction: Phishing\n",
            "\n",
            "Average Prediction: 0.0000\n",
            "Mode Prediction: 0\n",
            "Final Prediction (based on mode): Phishing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JL650EO89yW"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "# from urllib.parse import urlparse, urljoin\n",
        "# import pickle\n",
        "# import numpy as np\n",
        "# import re\n",
        "\n",
        "# def extract_features(url):\n",
        "#     features = {\n",
        "#         'LineOfCode': 0,\n",
        "#         'LargestLineLength': 0,\n",
        "#         'NoOfSelfRef': 0,\n",
        "#         'IsHTTPS': 1 if url.startswith(\"https\") else 0,\n",
        "#         'NoOfExternalRef': 0,\n",
        "#         'NoOfImage': 0,\n",
        "#         'NoOfJS': 0\n",
        "#     }\n",
        "\n",
        "#     try:\n",
        "#         response = requests.get(url)\n",
        "#         response.raise_for_status()\n",
        "\n",
        "#         lines = response.text.split('\\n')\n",
        "#         features['LineOfCode'] = len(lines)\n",
        "#         features['LargestLineLength'] = max(len(line) for line in lines)\n",
        "\n",
        "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
        "#         domain = urlparse(url).netloc\n",
        "\n",
        "#         for link in soup.find_all('a', href=True):\n",
        "#             href = link['href']\n",
        "#             full_url = urljoin(url, href)\n",
        "#             if urlparse(full_url).netloc == domain:\n",
        "#                 features['NoOfSelfRef'] += 1\n",
        "#             else:\n",
        "#                 features['NoOfExternalRef'] += 1\n",
        "\n",
        "#         features['NoOfImage'] = len(soup.find_all('img'))\n",
        "#         features['NoOfJS'] = len(soup.find_all('script'))\n",
        "\n",
        "#     except requests.RequestException as e:\n",
        "#         print(f\"Error fetching the webpage: {e}\")\n",
        "\n",
        "#     return features\n",
        "\n",
        "# # Load models from pickle files\n",
        "# def load_models():\n",
        "#     models = {}\n",
        "#     model_names = [\"logistic_regression.pkl\", \"decision_tree.pkl\", \"random_forest.pkl\",\"k-nearest_neighbors.pkl\",\"naive_bayes.pkl\", \"neural_network.pkl\", \"svm.pkl\"]\n",
        "\n",
        "#     for model_name in model_names:\n",
        "#         try:\n",
        "#             with open(model_name, 'rb') as f:\n",
        "#                 models[model_name] = pickle.load(f)\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error loading {model_name}: {e}\")\n",
        "\n",
        "#     return models\n",
        "\n",
        "# # Get user input\n",
        "# url = input(\"Enter the URL: \")\n",
        "# features = extract_features(url)\n",
        "# feature_values = np.array([list(features.values())]).reshape(1, -1)\n",
        "\n",
        "# models = load_models()\n",
        "\n",
        "# # Make predictions\n",
        "# for name, model in models.items():\n",
        "#     prediction = model.predict(feature_values)\n",
        "#     print(f\"{name}: {'Phishing' if prediction[0] == 0 else 'Legitimate'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "# from urllib.parse import urlparse, urljoin\n",
        "# import pickle\n",
        "# import numpy as np\n",
        "# import statistics\n",
        "# import time\n",
        "\n",
        "# def extract_features(url):\n",
        "#     features = {\n",
        "#         'LineOfCode': 0,\n",
        "#         'LargestLineLength': 0,\n",
        "#         'NoOfSelfRef': 0,\n",
        "#         'IsHTTPS': 1 if url.startswith(\"https\") else 0,\n",
        "#         'NoOfExternalRef': 0,\n",
        "#         'NoOfImage': 0,\n",
        "#         'NoOfJS': 0\n",
        "#     }\n",
        "\n",
        "#     try:\n",
        "#         for _ in range(3):  # Retry up to 3 times\n",
        "#             response = requests.get(url)\n",
        "#             if response.status_code == 200:\n",
        "#                 break\n",
        "#             time.sleep(2)\n",
        "#         else:\n",
        "#             print(f\"Error fetching the webpage: {response.status_code} {response.reason}\")\n",
        "#             return features\n",
        "\n",
        "#         response.raise_for_status()\n",
        "\n",
        "#         lines = response.text.split('\\n')\n",
        "#         features['LineOfCode'] = len(lines)\n",
        "#         features['LargestLineLength'] = max(len(line) for line in lines)\n",
        "\n",
        "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
        "#         domain = urlparse(url).netloc\n",
        "\n",
        "#         for link in soup.find_all('a', href=True):\n",
        "#             href = link['href']\n",
        "#             full_url = urljoin(url, href)\n",
        "#             if urlparse(full_url).netloc == domain:\n",
        "#                 features['NoOfSelfRef'] += 1\n",
        "#             else:\n",
        "#                 features['NoOfExternalRef'] += 1\n",
        "\n",
        "#         features['NoOfImage'] = len(soup.find_all('img'))\n",
        "#         features['NoOfJS'] = len(soup.find_all('script'))\n",
        "\n",
        "#     except requests.RequestException as e:\n",
        "#         print(f\"Error fetching the webpage: {e}\")\n",
        "\n",
        "#     return features\n",
        "\n",
        "# def load_models():\n",
        "#     models = {}\n",
        "#     model_names = [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"K-Nearest Neighbors\", \"Naive Bayes\", \"Neural Network\", \"SVM\"]\n",
        "\n",
        "#     for name in model_names:\n",
        "#         filename = f\"{name.replace(' ', '_').lower()}.pkl\"\n",
        "#         try:\n",
        "#             with open(filename, 'rb') as f:\n",
        "#                 models[name] = pickle.load(f)\n",
        "#         except FileNotFoundError:\n",
        "#             print(f\"Warning: {filename} not found. Skipping this model.\")\n",
        "\n",
        "#     return models\n",
        "\n",
        "# def predict_url(url, models):\n",
        "#     features = extract_features(url)\n",
        "\n",
        "#     # Display extracted features\n",
        "#     print(\"\\nExtracted Features:\")\n",
        "#     for key, value in features.items():\n",
        "#         print(f\"{key}: {value}\")\n",
        "\n",
        "#     feature_values = np.array(list(features.values())).reshape(1, -1)\n",
        "\n",
        "#     predictions = []\n",
        "#     for name, model in models.items():\n",
        "#         try:\n",
        "#             pred = model.predict(feature_values)[0]\n",
        "#             predictions.append(pred)\n",
        "#             print(f\"{name} Prediction: {'Legitimate' if pred else 'Phishing'}\")\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error predicting with {name}: {e}\")\n",
        "\n",
        "#     if predictions:\n",
        "#         avg_prediction = np.mean(predictions)\n",
        "#         mode_prediction = statistics.mode(predictions)\n",
        "\n",
        "#         final_prediction = 'Legitimate' if mode_prediction == 1 else 'Phishing'\n",
        "#         print(f\"\\nAverage Prediction: {avg_prediction:.4f}\")\n",
        "#         print(f\"Mode Prediction: {mode_prediction}\")\n",
        "#         print(f\"Final Prediction (based on mode): {final_prediction}\")\n",
        "#     else:\n",
        "#         print(\"No models available for prediction.\")\n",
        "\n",
        "# # Get URL from user and predict\n",
        "# url = input(\"Enter URL to classify: \")\n",
        "# models = load_models()\n",
        "# predict_url(url, models)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrHtDpOZKdvE",
        "outputId": "9a3440c3-efb5-4d17-af5c-71f306cc91ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter URL to classify: https://www.amazon.in/\n",
            "\n",
            "Extracted Features:\n",
            "LineOfCode: 3011\n",
            "LargestLineLength: 7131\n",
            "NoOfSelfRef: 86\n",
            "IsHTTPS: 1\n",
            "NoOfExternalRef: 17\n",
            "NoOfImage: 18\n",
            "NoOfJS: 60\n",
            "Logistic Regression Prediction: Legitimate\n",
            "Decision Tree Prediction: Legitimate\n",
            "Random Forest Prediction: Legitimate\n",
            "K-Nearest Neighbors Prediction: Legitimate\n",
            "Naive Bayes Prediction: Legitimate\n",
            "Neural Network Prediction: Legitimate\n",
            "SVM Prediction: Legitimate\n",
            "\n",
            "Average Prediction: 1.0000\n",
            "Mode Prediction: 1\n",
            "Final Prediction (based on mode): Legitimate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}